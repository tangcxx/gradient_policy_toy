{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424fc8e6-e309-4608-b270-a4fd7161eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.models import Model, clone_model\n",
    "from itertools import product\n",
    "\n",
    "import functions as fs\n",
    "from functions import make_state, run, get_optimal_value, get_optimal_action, get_optimal_actions, get_model_actions, test_model, one_batch, test_model_accuracy, one_batch_supervised, train, train_supervised, create_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5d4ac-5549-4dc6-b7cc-97e21e379fcb",
   "metadata": {},
   "source": [
    "# 单步奖励收集\n",
    "\n",
    "这是一个用于练习策略梯度法 (policy gradient method) 的玩具题。  \n",
    "8个格子首尾相连，其中随机3个格子奖励，奖励数值为0~1的均匀随机数。  \n",
    "动作：选择一个格子。\n",
    "计分规则：  \n",
    "1. 如果格子有奖励（大于0），获得对应的奖励。\n",
    "2. 如果格子是空的（等于0），且两侧的格子都有奖励，会获得两侧格子的总奖励。\n",
    "3. 否则奖励为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb980bc-15d7-4457-aaea-b4312895c248",
   "metadata": {},
   "source": [
    "## 试验场"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00400e9d-e300-49aa-acb8-346fc82940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_hidden_layers=1, n_dense_units=512, ratio_dropout=0.5, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5697b6-cdc2-4bcc-aec8-016d8dd689b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4196964480362878\n",
      "1 0.533370590878932\n",
      "2 0.5609241545276226\n",
      "3 0.5788089194138847\n",
      "4 0.6162808826571229\n",
      "5 0.6452382955228987\n",
      "6 0.655994152861143\n",
      "7 0.6722184017389673\n",
      "8 0.6987733853780318\n",
      "9 0.7171190232594449\n",
      "10 0.7223539906471552\n",
      "11 0.7033830109012923\n",
      "12 0.6959905332599443\n",
      "13 0.7044713307648616\n",
      "14 0.7143880538038173\n",
      "15 0.7154319676800828\n",
      "16 0.7097366429164729\n",
      "17 0.7067603394464773\n",
      "18 0.7058744872277171\n",
      "19 0.7443789022555416\n",
      "20 0.7775803586761424\n",
      "21 0.7762757632392603\n",
      "22 0.7747033627363588\n",
      "23 0.7781361266069076\n",
      "24 0.783626005851184\n",
      "25 0.788986649470874\n",
      "26 0.7768568303450715\n",
      "27 0.7676763491307047\n",
      "28 0.7875435922075503\n",
      "29 0.7685812257518093\n",
      "30 0.7461966768516559\n",
      "31 0.7541715711196213\n",
      "32 0.7439933785816666\n",
      "33 0.7635545243688332\n",
      "34 0.7953988900566931\n",
      "35 0.7802354797650374\n",
      "36 0.7768875566808764\n",
      "37 0.7761849346229717\n",
      "38 0.8079974398336426\n",
      "39 0.8120313718200692\n",
      "40 0.8113655419591916\n",
      "41 0.8078333821925159\n",
      "42 0.8117695504163437\n",
      "43 0.8177050332178093\n",
      "44 0.808231075415247\n",
      "45 0.7989502032726911\n",
      "46 0.792618518044555\n",
      "47 0.813652076872137\n",
      "48 0.8321813346671677\n",
      "49 0.841282360638275\n",
      "50 0.8476112883990511\n",
      "51 0.8477998757267278\n",
      "52 0.845422005285235\n",
      "53 0.8509225453830664\n",
      "54 0.8494037787327068\n",
      "55 0.8538386350075999\n",
      "56 0.8503462984663182\n",
      "57 0.8424033973958649\n",
      "58 0.8386997461294683\n",
      "59 0.8451211238803955\n",
      "60 0.8442249196589459\n",
      "61 0.8347795167270524\n",
      "62 0.8320930611157602\n",
      "63 0.8209341383801526\n",
      "64 0.8429942091032983\n",
      "65 0.8605096654400309\n",
      "66 0.8709528278785102\n",
      "67 0.8588344520103474\n",
      "68 0.8689210454352114\n",
      "69 0.8605929054310429\n",
      "70 0.8449337314309542\n",
      "71 0.8406471109494078\n",
      "72 0.8359192947157844\n",
      "73 0.8367065811044093\n",
      "74 0.8299728883664695\n",
      "75 0.825526151269941\n",
      "76 0.8380819454234419\n",
      "77 0.8733541660347313\n",
      "78 0.8649333857001011\n",
      "79 0.8608854001082884\n",
      "80 0.8674556528233696\n",
      "81 0.8788897877332024\n",
      "82 0.8684807622657703\n",
      "83 0.8642534958802726\n",
      "84 0.870605171482725\n",
      "85 0.8773658556998538\n",
      "86 0.8696551117669855\n",
      "87 0.8848176114065891\n",
      "88 0.892805906106112\n",
      "89 0.8909105529418002\n",
      "90 0.887261607180623\n",
      "91 0.8773898264785861\n",
      "92 0.8530457303778086\n",
      "93 0.8342775835287024\n",
      "94 0.8389549079115748\n",
      "95 0.8391738345417751\n",
      "96 0.8641904099893359\n",
      "97 0.8605063896224961\n",
      "98 0.8671178306040085\n",
      "99 0.8585680428116405\n",
      "100 0.8660848576541582\n",
      "101 0.8779777947761644\n",
      "102 0.8643363406367793\n",
      "103 0.854645786707381\n",
      "104 0.8498878970264863\n",
      "105 0.8424273935866473\n",
      "106 0.8377834553592686\n",
      "107 0.8417996764716852\n",
      "108 0.8390559431516904\n",
      "109 0.8399941956952917\n",
      "110 0.8534782450660253\n",
      "111 0.8496951482339519\n",
      "112 0.8527753901241889\n",
      "113 0.8414699823822919\n",
      "114 0.8279022025092776\n",
      "115 0.8316935879535374\n",
      "116 0.8266913182483023\n",
      "117 0.8340392881544929\n",
      "118 0.8454573363448575\n",
      "119 0.8412254739783395\n",
      "120 0.828975991602201\n",
      "121 0.8424888342597356\n",
      "122 0.8450707936516119\n",
      "123 0.8394691220887104\n",
      "124 0.8343107150926838\n",
      "125 0.8143307684229346\n",
      "126 0.8218994573206383\n",
      "127 0.8338551751404925\n",
      "128 0.8470066986183894\n",
      "129 0.8453128917968166\n",
      "130 0.8563513154376448\n",
      "131 0.8632141340953785\n",
      "132 0.8616850860254645\n",
      "133 0.8564163462602379\n",
      "134 0.8446580171014477\n",
      "135 0.8222275337096465\n",
      "136 0.8386777196240549\n",
      "137 0.8266104265672664\n",
      "138 0.8429279340939798\n",
      "139 0.8358891312885973\n",
      "140 0.8460157567045308\n",
      "141 0.8362535160824609\n",
      "142 0.8458910606483924\n",
      "143 0.8343304958970471\n",
      "144 0.847297297953111\n",
      "145 0.8508198074058886\n",
      "146 0.8397662031881511\n",
      "147 0.8412758688631591\n",
      "148 0.8575900165205139\n",
      "149 0.8517091772574223\n",
      "150 0.8552117817280452\n",
      "151 0.8533319928031966\n",
      "152 0.8430447440327363\n",
      "153 0.8462160645509217\n",
      "154 0.8397268425132457\n",
      "155 0.8456334930262086\n",
      "156 0.8463482351284958\n",
      "157 0.8510830999074648\n",
      "158 0.8388360814930428\n",
      "159 0.8328956252156055\n",
      "160 0.8240874440204958\n",
      "161 0.8193795373918027\n",
      "162 0.8366898927677179\n",
      "163 0.8265150195586961\n",
      "164 0.8244726161062339\n",
      "165 0.82429251247494\n",
      "166 0.8191770357257351\n",
      "167 0.8207263186315295\n",
      "168 0.8152613037802731\n",
      "169 0.8137601484303253\n",
      "170 0.8261926084096131\n",
      "171 0.8125262569274609\n",
      "172 0.8166028142238025\n",
      "173 0.7918849742382725\n",
      "174 0.7968230387295163\n",
      "175 0.8052813534692334\n",
      "176 0.8150369628085368\n",
      "177 0.8093774885841798\n",
      "178 0.8108724509142132\n",
      "179 0.8133976002494592\n",
      "180 0.8123028743569662\n",
      "181 0.8095849645264477\n",
      "182 0.7928184499082199\n",
      "183 0.7877986790102685\n",
      "184 0.8046423210618091\n",
      "185 0.8015755789894643\n",
      "186 0.816496705781973\n",
      "187 0.7937524855944733\n",
      "188 0.7893558370603487\n",
      "189 0.7804869501175666\n",
      "190 0.7909680147307622\n",
      "191 0.7940823631595632\n",
      "192 0.8032745189792445\n",
      "193 0.797395170290001\n",
      "194 0.792224144194764\n",
      "195 0.7978477797725425\n",
      "196 0.7951062481226178\n",
      "197 0.7875460572720745\n",
      "198 0.7731596824780286\n",
      "199 0.7789151394052094\n"
     ]
    }
   ],
   "source": [
    "best_idx, best_score, best_weights = train(model, 200, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bdb4d3-a443-49dd-b6b8-8bd0e2eb180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(88), np.float64(0.892805906106112))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a5de1f-2cb8-4b95-be07-5593fdd89037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cdf3e29-9b5e-45f2-bd9a-ddb35e5d7d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8903345454723381)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b7f914-5df0-42f7-ba3f-66fd067fedbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4959)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_accuracy(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12357dc1-dffc-4cfe-bb2c-a43f68df6fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c4e53-113d-4c3e-9313-b49226fab9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360bff5-18f0-4498-8f39-609d65a87fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f5badc-000b-49c3-8003-67d7c2702813",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_supervised = create_model(n_hidden_layers=2, n_dense_units=512, ratio_dropout=0.5, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66380e68-8d74-4d87-a140-1239c35d4888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5798255174581393\n",
      "1 0.6715001091366704\n",
      "2 0.6762108837108425\n",
      "3 0.7180279808981633\n",
      "4 0.7237650492489376\n",
      "5 0.7151350099839339\n",
      "6 0.7058486956482112\n",
      "7 0.7009318060557419\n",
      "8 0.7166195915181409\n",
      "9 0.7583921386864083\n",
      "10 0.7796342298048112\n",
      "11 0.7991952385965791\n",
      "12 0.7865246071069473\n",
      "13 0.7863458954037722\n",
      "14 0.7921531220020768\n",
      "15 0.8020982985265503\n",
      "16 0.8050563632421734\n",
      "17 0.7843381442977807\n",
      "18 0.7783986838224585\n",
      "19 0.7662579167319429\n",
      "20 0.7735607376108803\n",
      "21 0.7683413114838522\n",
      "22 0.7772922221070516\n",
      "23 0.7752406756752738\n",
      "24 0.7658300852460661\n",
      "25 0.7626813411891772\n",
      "26 0.7643539313020212\n",
      "27 0.745547003197266\n",
      "28 0.7619439461651285\n",
      "29 0.7573852882142701\n",
      "30 0.7729987892947844\n",
      "31 0.7829905282565662\n",
      "32 0.7799546308585512\n",
      "33 0.7783302276314537\n",
      "34 0.7750431488453194\n",
      "35 0.7579681585722092\n",
      "36 0.7908543785272706\n",
      "37 0.8123053456390713\n",
      "38 0.8174631549771002\n",
      "39 0.8199705108491878\n",
      "40 0.8100872079188145\n",
      "41 0.8066552138558288\n",
      "42 0.8113612039691162\n",
      "43 0.8031792989392078\n",
      "44 0.802376837407698\n",
      "45 0.8035674866996085\n",
      "46 0.8050351619798929\n",
      "47 0.8078879121840877\n",
      "48 0.8156997023483773\n",
      "49 0.823365099485806\n",
      "50 0.8200409515532224\n",
      "51 0.8183995146896988\n",
      "52 0.8328007212551396\n",
      "53 0.8288511508625052\n",
      "54 0.8439521077792626\n",
      "55 0.8417262593290729\n",
      "56 0.8493266767351514\n",
      "57 0.8385191321639471\n",
      "58 0.8481251109396782\n",
      "59 0.8566624182997015\n",
      "60 0.859532765359352\n",
      "61 0.8626253497956128\n",
      "62 0.8623657110309206\n",
      "63 0.8629497600671086\n",
      "64 0.8572033204750492\n",
      "65 0.850418077865048\n",
      "66 0.8474168401380224\n",
      "67 0.8485676196072262\n",
      "68 0.8529886685159006\n",
      "69 0.8529578234316414\n",
      "70 0.866445672508899\n",
      "71 0.8862753949512142\n",
      "72 0.8841833381206404\n",
      "73 0.8763922798635924\n",
      "74 0.8617692400804328\n",
      "75 0.8579819909291558\n",
      "76 0.8695983201747441\n",
      "77 0.8677388250257192\n",
      "78 0.8772880501118032\n",
      "79 0.8817949243808832\n",
      "80 0.8847865132389335\n",
      "81 0.8806201448949913\n",
      "82 0.8855540685556549\n",
      "83 0.8835833864853397\n",
      "84 0.8824681035386206\n",
      "85 0.8891450754830786\n",
      "86 0.883551486037088\n",
      "87 0.8886328902835743\n",
      "88 0.8954077452727907\n",
      "89 0.8872679967578871\n",
      "90 0.8912268113573043\n",
      "91 0.8993631119004393\n",
      "92 0.8936250909751665\n",
      "93 0.8909718030466566\n",
      "94 0.8892613894285701\n",
      "95 0.8933213519285036\n",
      "96 0.898340509371423\n",
      "97 0.900879922748424\n",
      "98 0.8938633701326937\n",
      "99 0.894311698288973\n",
      "100 0.8964684785550342\n",
      "101 0.8966758979879358\n",
      "102 0.8934697114617087\n",
      "103 0.9045782407602951\n",
      "104 0.8963977995856601\n",
      "105 0.8940706273348982\n",
      "106 0.9067064173480367\n",
      "107 0.8984705365856542\n",
      "108 0.8985287023400382\n",
      "109 0.8989886955499151\n",
      "110 0.9036241104962839\n",
      "111 0.9037005034347579\n",
      "112 0.8951869037961827\n",
      "113 0.8951415400848095\n",
      "114 0.8991147329888172\n",
      "115 0.8949937508535709\n",
      "116 0.9006689238246856\n",
      "117 0.9045947791829282\n",
      "118 0.9044430141229702\n",
      "119 0.8966559208086617\n",
      "120 0.8993066673823205\n",
      "121 0.8993603194667392\n",
      "122 0.9015250949390807\n",
      "123 0.904809358938969\n",
      "124 0.8962010480433488\n",
      "125 0.9024625638978163\n",
      "126 0.9025391965982779\n",
      "127 0.8963639550000447\n",
      "128 0.902386867255326\n",
      "129 0.9078275941404205\n",
      "130 0.906801429451182\n",
      "131 0.8995215282638875\n",
      "132 0.8969919164705963\n",
      "133 0.9045614103359724\n",
      "134 0.897997072033044\n",
      "135 0.9000359529883115\n",
      "136 0.8989958802545809\n",
      "137 0.9020003710598504\n",
      "138 0.8968313832434204\n",
      "139 0.9060803059685293\n",
      "140 0.9034331924287002\n",
      "141 0.9036351360238263\n",
      "142 0.9056404508447089\n",
      "143 0.9041064272461392\n",
      "144 0.9055868778614761\n",
      "145 0.9070328355555787\n",
      "146 0.9035561006806123\n",
      "147 0.9033182362380682\n",
      "148 0.9032551518497756\n",
      "149 0.8982240150152219\n",
      "150 0.90282558777623\n",
      "151 0.9030210168887693\n",
      "152 0.9021939202107294\n",
      "153 0.9070805114616938\n",
      "154 0.9021566746256241\n",
      "155 0.9094485712686412\n",
      "156 0.9055931743841773\n",
      "157 0.9038475576621149\n",
      "158 0.9089460779482184\n",
      "159 0.9092630015213586\n",
      "160 0.9097557725438786\n",
      "161 0.8979412825443535\n",
      "162 0.9041067757589919\n",
      "163 0.9044275139438405\n",
      "164 0.9100531729972671\n",
      "165 0.9096420419755153\n",
      "166 0.9074563698416358\n",
      "167 0.9056906868922567\n",
      "168 0.9037827977263678\n",
      "169 0.9037105087533437\n",
      "170 0.9086369223267752\n",
      "171 0.9092932087346957\n",
      "172 0.9122070076110748\n",
      "173 0.9080230050190728\n",
      "174 0.9142909784299929\n",
      "175 0.9145854174961063\n",
      "176 0.9154613052621853\n",
      "177 0.9150751291651159\n",
      "178 0.9066224288295823\n",
      "179 0.9084753414869101\n",
      "180 0.9043964227249311\n",
      "181 0.9056723450786852\n",
      "182 0.9080878821487999\n",
      "183 0.9060967653645728\n",
      "184 0.9089780797322575\n",
      "185 0.9031926000856951\n",
      "186 0.9070465861116721\n",
      "187 0.9040945131767448\n",
      "188 0.9082727134822226\n",
      "189 0.9090423029192214\n",
      "190 0.9060412044664159\n",
      "191 0.9104150494036094\n",
      "192 0.9145558344218735\n",
      "193 0.9070147130332713\n",
      "194 0.9016870161305118\n",
      "195 0.9151418671869762\n",
      "196 0.9123937219940886\n",
      "197 0.9102274921129876\n",
      "198 0.9044100029795662\n",
      "199 0.9097844318055993\n"
     ]
    }
   ],
   "source": [
    "train_supervised(model_supervised, 200, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2a3782a4-0f3e-404c-ab31-0c7c2f071723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9005773898710413)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model_supervised, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5e76038b-64ca-4c00-a1d1-046fd8c2f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.769)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_accuracy(model_supervised, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1c4ac-cba0-4a8a-ac78-219395bc1954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51526c42-3033-4bd4-901e-47736be9a71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9a23a38-6743-4a25-82f8-01bf9e9bc5e6",
   "metadata": {},
   "source": [
    "## 一些测试结论\n",
    "\n",
    "先做了一些模拟测试（10万次）确定评价模型的基准。如果采用最优解，单局游戏奖励的平均值为0.933，标准差0.347。采用随机策略的平均奖励是0.297。\n",
    "\n",
    "首先初步对比了一下有无 BatchNormalization 和 Dropout 的情况，差距颇大。无 BatchNormalization 和 Dropout 时表现很差。观察 weights，应该是遇到了梯度消失和梯度爆炸。\n",
    "\n",
    "之后的测试都用了 BatchNormalization 和 Dropout。\n",
    "\n",
    "依次是：optimalizer, n_hidden_layers, n_dense_units, ratio_dropout, argmax(scores), max(scores)，最后两列是监督学习的结果\n",
    "sgd\t1\t64\t0.2\t230\t0.81957047\t199\t0.814762256\n",
    "sgd\t1\t64\t0.3\t133\t0.823306626\t232\t0.846447604\n",
    "sgd\t1\t64\t0.4\t215\t0.809455314\t199\t0.842190759\n",
    "sgd\t1\t64\t0.5\t222\t0.856119525\t285\t0.844487367\n",
    "sgd\t1\t64\t0.6\t219\t0.865266425\t216\t0.833977499\n",
    "sgd\t1\t64\t0.7\t257\t0.817717295\t233\t0.810288408\n",
    "sgd\t1\t128\t0.2\t138\t0.848355803\t225\t0.834754306\n",
    "sgd\t1\t128\t0.3\t233\t0.843209246\t201\t0.837043901\n",
    "sgd\t1\t128\t0.4\t203\t0.84398221\t286\t0.842715486\n",
    "sgd\t1\t128\t0.5\t123\t0.884888732\t265\t0.841131572\n",
    "sgd\t1\t128\t0.6\t222\t0.832868491\t185\t0.844697325\n",
    "sgd\t1\t128\t0.7\t137\t0.857809365\t222\t0.83471746\n",
    "sgd\t1\t256\t0.2\t197\t0.878311576\t162\t0.891374889\n",
    "sgd\t1\t256\t0.3\t155\t0.879151885\t154\t0.878041714\n",
    "sgd\t1\t256\t0.4\t241\t0.860009212\t211\t0.884917987\n",
    "sgd\t1\t256\t0.5\t146\t0.878039385\t88\t0.877335078\n",
    "sgd\t1\t256\t0.6\t199\t0.874558249\t155\t0.872676206\n",
    "sgd\t1\t256\t0.7\t184\t0.852593144\t193\t0.867367797\n",
    "sgd\t1\t512\t0.2\t144\t0.900807575\t155\t0.881127728\n",
    "sgd\t1\t512\t0.3\t157\t0.903435378\t185\t0.902059197\n",
    "sgd\t1\t512\t0.4\t156\t0.896634056\t148\t0.891186373\n",
    "sgd\t1\t512\t0.5\t111\t0.885416902\t167\t0.887317218\n",
    "sgd\t1\t512\t0.6\t145\t0.881211699\t179\t0.896044479\n",
    "sgd\t1\t512\t0.7\t195\t0.880886037\t171\t0.878781404\n",
    "sgd\t2\t64\t0.2\t180\t0.829470364\t213\t0.824875833\n",
    "sgd\t2\t64\t0.3\t213\t0.813004653\t181\t0.810666407\n",
    "sgd\t2\t64\t0.4\t221\t0.796345613\t165\t0.8167897\n",
    "sgd\t2\t64\t0.5\t215\t0.82388745\t238\t0.82007955\n",
    "sgd\t2\t64\t0.6\t195\t0.810177596\t216\t0.806053396\n",
    "sgd\t2\t64\t0.7\t262\t0.77908087\t221\t0.781639501\n",
    "sgd\t2\t128\t0.2\t168\t0.810202247\t202\t0.829993644\n",
    "sgd\t2\t128\t0.3\t160\t0.849377718\t142\t0.852961319\n",
    "sgd\t2\t128\t0.4\t184\t0.818962699\t168\t0.797151086\n",
    "sgd\t2\t128\t0.5\t216\t0.823738145\t153\t0.822228704\n",
    "sgd\t2\t128\t0.6\t170\t0.830139236\t185\t0.815216612\n",
    "sgd\t2\t128\t0.7\t225\t0.808585633\t162\t0.831086245\n",
    "sgd\t2\t256\t0.2\t223\t0.821159954\t128\t0.825304977\n",
    "sgd\t2\t256\t0.3\t168\t0.833991425\t213\t0.814544642\n",
    "sgd\t2\t256\t0.4\t201\t0.806464947\t179\t0.810070847\n",
    "sgd\t2\t256\t0.5\t131\t0.833720606\t172\t0.823969548\n",
    "sgd\t2\t256\t0.6\t143\t0.833717145\t125\t0.840917363\n",
    "sgd\t2\t256\t0.7\t168\t0.800307924\t216\t0.81568297\n",
    "sgd\t2\t512\t0.2\t148\t0.814656652\t163\t0.804054857\n",
    "sgd\t2\t512\t0.3\t175\t0.815816799\t170\t0.797978322\n",
    "sgd\t2\t512\t0.4\t204\t0.802997249\t143\t0.787797558\n",
    "sgd\t2\t512\t0.5\t188\t0.787115627\t133\t0.82032757\n",
    "sgd\t2\t512\t0.6\t148\t0.831742408\t138\t0.843009766\n",
    "sgd\t2\t512\t0.7\t177\t0.810344541\t125\t0.825383229\n",
    "adam\t1\t64\t0.2\t237\t0.800325885\t106\t0.851902918\n",
    "adam\t1\t64\t0.3\t110\t0.83046924\t145\t0.841763447\n",
    "adam\t1\t64\t0.4\t137\t0.830828595\t152\t0.87059485\n",
    "adam\t1\t64\t0.5\t257\t0.850197804\t135\t0.832163489\n",
    "adam\t1\t64\t0.6\t246\t0.848341135\t235\t0.826210482\n",
    "adam\t1\t64\t0.7\t297\t0.820871104\t272\t0.840769845\n",
    "adam\t1\t128\t0.2\t73\t0.872352655\t79\t0.840301851\n",
    "adam\t1\t128\t0.3\t120\t0.852311373\t99\t0.852244187\n",
    "adam\t1\t128\t0.4\t147\t0.87223051\t88\t0.860748639\n",
    "adam\t1\t128\t0.5\t158\t0.868205929\t95\t0.870478781\n",
    "adam\t1\t128\t0.6\t140\t0.877279833\t119\t0.893925984\n",
    "adam\t1\t128\t0.7\t169\t0.866685221\t236\t0.85733735\n",
    "adam\t1\t256\t0.2\t83\t0.859628576\t95\t0.866258281\n",
    "adam\t1\t256\t0.3\t83\t0.876542719\t78\t0.868539565\n",
    "adam\t1\t256\t0.4\t93\t0.894967784\t91\t0.889777236\n",
    "adam\t1\t256\t0.5\t83\t0.877598209\t100\t0.885931925\n",
    "adam\t1\t256\t0.6\t123\t0.894390356\t98\t0.893189216\n",
    "adam\t1\t256\t0.7\t152\t0.87269044\t143\t0.890608802\n",
    "adam\t1\t512\t0.2\t62\t0.862585047\t70\t0.886787742\n",
    "adam\t1\t512\t0.3\t83\t0.884045646\t72\t0.878337055\n",
    "adam\t1\t512\t0.4\t97\t0.878438115\t66\t0.880801083\n",
    "adam\t1\t512\t0.5\t94\t0.896152579\t72\t0.896046623\n",
    "adam\t1\t512\t0.6\t91\t0.889568521\t102\t0.905443719\n",
    "adam\t1\t512\t0.7\t97\t0.895427823\t121\t0.906246792\n",
    "adam\t2\t64\t0.2\t164\t0.757905701\t271\t0.733956743\n",
    "adam\t2\t64\t0.3\t110\t0.796601826\t140\t0.718582735\n",
    "adam\t2\t64\t0.4\t88\t0.749958685\t107\t0.806369852\n",
    "adam\t2\t64\t0.5\t121\t0.793363862\t152\t0.800517474\n",
    "adam\t2\t64\t0.6\t118\t0.802693139\t124\t0.782237715\n",
    "adam\t2\t64\t0.7\t165\t0.735777228\t199\t0.782845587\n",
    "adam\t2\t128\t0.2\t291\t0.774913456\t110\t0.722267079\n",
    "adam\t2\t128\t0.3\t91\t0.757426543\t105\t0.790650053\n",
    "adam\t2\t128\t0.4\t96\t0.794715882\t129\t0.799408182\n",
    "adam\t2\t128\t0.5\t104\t0.786028039\t122\t0.787805528\n",
    "adam\t2\t128\t0.6\t108\t0.835592486\t119\t0.799569394\n",
    "adam\t2\t128\t0.7\t109\t0.791363958\t106\t0.812970546\n",
    "adam\t2\t256\t0.2\t181\t0.767824652\t165\t0.776558887\n",
    "adam\t2\t256\t0.3\t193\t0.769441759\t289\t0.773747808\n",
    "adam\t2\t256\t0.4\t99\t0.790121204\t251\t0.754174716\n",
    "adam\t2\t256\t0.5\t106\t0.772241874\t96\t0.822737305\n",
    "adam\t2\t256\t0.6\t143\t0.78152916\t95\t0.784852849\n",
    "adam\t2\t256\t0.7\t126\t0.797849688\t101\t0.762861092\n",
    "adam\t2\t512\t0.2\t241\t0.752311116\t125\t0.774890214\n",
    "adam\t2\t512\t0.3\t149\t0.76133756\t122\t0.774810565\n",
    "adam\t2\t512\t0.4\t220\t0.777328298\t137\t0.762169157\n",
    "adam\t2\t512\t0.5\t224\t0.749306438\t155\t0.769043194\n",
    "adam\t2\t512\t0.6\t237\t0.749582459\t106\t0.769278361\n",
    "adam\t2\t512\t0.7\t98\t0.789538087\t167\t0.725196807\n",
    "\n",
    "\n",
    "几个结论：\n",
    "1. 对比强化学习和监督学习，强化学习算法的实现方式是正确的。\n",
    "2. adam 比 sgd 更快，且更稳定（没有体现在上面的数据里），为什么？sgd模型的参数绝对值会越来越大，adam似乎没这个问题\n",
    "3. 对于两个隐藏层的模型，sgd 比 adam 表现更好\n",
    "4. 两个隐藏层的模型比一个隐藏层的模型效果差。为什么？\n",
    "5. 为什么增大模型规模能提升模型效果和加快学习速度？看到过一个理论，说模型中大部分参数是没用的，只有部分参数起决定性作用，因为这部分参数的初始值很好。大的模型有更多的参数，有更大机会命中较好的初始参数。\n",
    "\n",
    "观察了两个强化学习和监督学习模型，各自的得分是0.89和0.9，差距很小，最优解的命中率却是0.49和0.77。考虑：\n",
    "1. 用 baseline 方法重新训练策略模型试试。\n",
    "2. 训练过程中逐步降温。\n",
    "\n",
    "之后调整测试 policy gradient method 采用超参数：optimalizer, n_hidden_layers, n_dense_units, ratio_dropout = 'adam', 1, 512, 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824bdb8-b2bd-48f6-97dd-3ce2f526a8ee",
   "metadata": {},
   "source": [
    "## baseline\n",
    "\n",
    "将 one_batch 中 \n",
    "```\n",
    "y_target[action] = run(state, action)\n",
    "```\n",
    "改为\n",
    "```\n",
    "y_target[action] = run(state, action) - 0.933\n",
    "```\n",
    "\n",
    "模型没有完全失效，训练过程中仍有提升，但只达到了0.72 左右。我看不出为什么在奖励上减去一个固定的数字会影响结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
