{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424fc8e6-e309-4608-b270-a4fd7161eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from itertools import product\n",
    "\n",
    "import functions as fs\n",
    "from functions import make_state, run, get_optimal_value, get_optimal_action, get_optimal_actions, get_model_actions, test_model, one_batch, test_model_accuracy, one_batch_supervised, train, train_supervised, create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5d4ac-5549-4dc6-b7cc-97e21e379fcb",
   "metadata": {},
   "source": [
    "# 单步奖励收集\n",
    "\n",
    "这是一个用于练习策略梯度法 (policy gradient method) 的玩具题。  \n",
    "8个格子首尾相连，其中随机3个格子奖励，奖励数值为0~1的均匀随机数。  \n",
    "动作：选择一个格子。\n",
    "计分规则：  \n",
    "1. 如果格子有奖励（大于0），获得对应的奖励。\n",
    "2. 如果格子是空的（等于0），且两侧的格子都有奖励，会获得两侧格子的总奖励。\n",
    "3. 否则奖励为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc104180-5426-44ec-a82b-4d864b068367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c2bd7ed-441c-4dc9-ae7a-5ce0a491f53f",
   "metadata": {},
   "source": [
    "## 测试监督学习效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddc9ea7-92b3-405b-b314-b70375d494be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd 1 64 0.2 290 0.8894605271764167\n",
      "sgd 1 64 0.3 295 0.8761614984673568\n",
      "sgd 1 64 0.4 289 0.8733202250553918\n",
      "sgd 1 64 0.5 292 0.877693469215966\n",
      "sgd 1 64 0.6 269 0.8733585605920671\n",
      "sgd 1 64 0.7 249 0.8671943778947623\n",
      "sgd 1 128 0.2 181 0.8969206804154661\n",
      "sgd 1 128 0.3 180 0.8960513484973998\n",
      "sgd 1 128 0.4 293 0.8957055927253046\n",
      "sgd 1 128 0.5 276 0.8953302068304051\n",
      "sgd 1 128 0.6 295 0.8924188652748765\n",
      "sgd 1 128 0.7 253 0.8941561621538368\n",
      "sgd 1 256 0.2 278 0.904177163572226\n",
      "sgd 1 256 0.3 221 0.9040284736918341\n",
      "sgd 1 256 0.4 260 0.9062705495465576\n",
      "sgd 1 256 0.5 253 0.903307252891115\n",
      "sgd 1 256 0.6 259 0.8997506592172958\n",
      "sgd 1 256 0.7 253 0.8993784495272655\n",
      "sgd 1 512 0.2 259 0.9131638545916596\n",
      "sgd 1 512 0.3 276 0.9097489778531546\n",
      "sgd 1 512 0.4 225 0.9079081757583483\n",
      "sgd 1 512 0.5 253 0.9111476706843749\n",
      "sgd 1 512 0.6 280 0.9075796399391469\n",
      "sgd 1 512 0.7 230 0.9063435648831448\n",
      "sgd 2 64 0.2 293 0.8947735987399619\n",
      "sgd 2 64 0.3 251 0.8798183790991799\n",
      "sgd 2 64 0.4 293 0.8822541154401481\n",
      "sgd 2 64 0.5 295 0.8536343606586763\n",
      "sgd 2 64 0.6 291 0.8466980503573305\n",
      "sgd 2 64 0.7 259 0.8263038076068169\n",
      "sgd 2 128 0.2 289 0.9062642254612227\n",
      "sgd 2 128 0.3 226 0.8999953172941699\n",
      "sgd 2 128 0.4 239 0.8931032676569371\n",
      "sgd 2 128 0.5 293 0.8884770150919269\n",
      "sgd 2 128 0.6 227 0.8754884642404228\n",
      "sgd 2 128 0.7 163 0.8619954090106945\n",
      "sgd 2 256 0.2 205 0.9111845273175583\n",
      "sgd 2 256 0.3 290 0.9089335814089674\n",
      "sgd 2 256 0.4 209 0.9050678951906616\n",
      "sgd 2 256 0.5 298 0.9018744172954173\n",
      "sgd 2 256 0.6 265 0.8931636711832897\n",
      "sgd 2 256 0.7 227 0.88257435258847\n",
      "sgd 2 512 0.2 267 0.9199628485227378\n",
      "sgd 2 512 0.3 256 0.9152133817888399\n",
      "sgd 2 512 0.4 278 0.9125672228989437\n",
      "sgd 2 512 0.5 269 0.9063368637444764\n",
      "sgd 2 512 0.6 294 0.9042145155838464\n",
      "sgd 2 512 0.7 256 0.8960722011622682\n",
      "adam 1 64 0.2 287 0.8961117251004327\n",
      "adam 1 64 0.3 287 0.8959841273057314\n",
      "adam 1 64 0.4 242 0.8965864165082763\n",
      "adam 1 64 0.5 264 0.8906625053480844\n",
      "adam 1 64 0.6 257 0.8753276992632325\n",
      "adam 1 64 0.7 249 0.8721354137711685\n",
      "adam 1 128 0.2 268 0.9077066578657094\n",
      "adam 1 128 0.3 288 0.9053896691642931\n",
      "adam 1 128 0.4 224 0.9010394747963041\n",
      "adam 1 128 0.5 266 0.900511726079033\n",
      "adam 1 128 0.6 230 0.8958621714024811\n",
      "adam 1 128 0.7 296 0.8931279859960579\n",
      "adam 1 256 0.2 298 0.9160275143470812\n",
      "adam 1 256 0.3 278 0.9120328552211228\n",
      "adam 1 256 0.4 273 0.9090487257316383\n",
      "adam 1 256 0.5 296 0.9111219616947155\n",
      "adam 1 256 0.6 246 0.9060362818631107\n",
      "adam 1 256 0.7 270 0.9028078805208355\n",
      "adam 1 512 0.2 295 0.9211059685631752\n",
      "adam 1 512 0.3 298 0.9168777706143525\n",
      "adam 1 512 0.4 258 0.915632516001249\n",
      "adam 1 512 0.5 270 0.9158815797974392\n",
      "adam 1 512 0.6 299 0.914139046607813\n",
      "adam 1 512 0.7 236 0.9112339443914363\n",
      "adam 2 64 0.2 252 0.9097529689172889\n",
      "adam 2 64 0.3 291 0.9036475147691897\n",
      "adam 2 64 0.4 264 0.8924796945708214\n",
      "adam 2 64 0.5 288 0.8753350545425707\n",
      "adam 2 64 0.6 101 0.8534699975069013\n",
      "adam 2 64 0.7 157 0.8451908788984801\n",
      "adam 2 128 0.2 239 0.9209312502880214\n",
      "adam 2 128 0.3 289 0.9138250317930399\n",
      "adam 2 128 0.4 283 0.9063089552853245\n",
      "adam 2 128 0.5 265 0.9003693627239402\n",
      "adam 2 128 0.6 291 0.8925800758417618\n",
      "adam 2 128 0.7 296 0.8654211024053867\n",
      "adam 2 256 0.2 244 0.9256248638291983\n",
      "adam 2 256 0.3 233 0.9237521876951438\n",
      "adam 2 256 0.4 164 0.9148531182769646\n",
      "adam 2 256 0.5 245 0.9096733893179356\n",
      "adam 2 256 0.6 216 0.9063327950222132\n",
      "adam 2 256 0.7 207 0.8980725318721993\n",
      "adam 2 512 0.2 189 0.9254844827991019\n",
      "adam 2 512 0.3 271 0.9279896555272724\n",
      "adam 2 512 0.4 237 0.9225782492977623\n",
      "adam 2 512 0.5 279 0.9149559263426945\n",
      "adam 2 512 0.6 298 0.9141901598970782\n",
      "adam 2 512 0.7 248 0.9107444079587558\n"
     ]
    }
   ],
   "source": [
    "for optimilizer in ['sgd', 'adam']:\n",
    "    for n_hidden_layers in [1, 2]:\n",
    "        for n_dense_units in [64, 128, 256, 512]:\n",
    "            for ratio_dropout in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "                model_supervised = create_model(n_hidden_layers, n_dense_units, ratio_dropout, optimilizer)\n",
    "                scores = np.array([one_batch_supervised(model_supervised, batch_size=128, n_test_rounds=10000) for i in np.arange(300)])\n",
    "                argmax = scores.argmax()\n",
    "                print(optimilizer, n_hidden_layers, n_dense_units, ratio_dropout, argmax, scores[argmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e3fe0-c5f1-4880-86e8-d4db3093a4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
