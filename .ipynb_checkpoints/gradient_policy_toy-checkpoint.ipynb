{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424fc8e6-e309-4608-b270-a4fd7161eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.models import Model, clone_model\n",
    "from itertools import product\n",
    "\n",
    "import functions as fs\n",
    "from functions import make_state, run, get_optimal_value, get_optimal_action, get_optimal_actions, get_model_actions, test_model, one_batch, test_model_accuracy, one_batch_supervised, train, train_supervised, create_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5d4ac-5549-4dc6-b7cc-97e21e379fcb",
   "metadata": {},
   "source": [
    "# 单步奖励收集\n",
    "\n",
    "这是一个用于练习策略梯度法 (policy gradient method) 的玩具题。  \n",
    "8个格子首尾相连，其中随机3个格子奖励，奖励数值为0~1的均匀随机数。  \n",
    "动作：选择一个格子。\n",
    "计分规则：  \n",
    "1. 如果格子有奖励（大于0），获得对应的奖励。\n",
    "2. 如果格子是空的（等于0），且两侧的格子都有奖励，会获得两侧格子的总奖励。\n",
    "3. 否则奖励为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb980bc-15d7-4457-aaea-b4312895c248",
   "metadata": {},
   "source": [
    "## 试验场"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00400e9d-e300-49aa-acb8-346fc82940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_hidden_layers=1, n_dense_units=512, ratio_dropout=0.5, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5697b6-cdc2-4bcc-aec8-016d8dd689b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4196964480362878\n",
      "1 0.533370590878932\n",
      "2 0.5609241545276226\n",
      "3 0.5788089194138847\n",
      "4 0.6162808826571229\n",
      "5 0.6452382955228987\n",
      "6 0.655994152861143\n",
      "7 0.6722184017389673\n",
      "8 0.6987733853780318\n",
      "9 0.7171190232594449\n",
      "10 0.7223539906471552\n",
      "11 0.7033830109012923\n",
      "12 0.6959905332599443\n",
      "13 0.7044713307648616\n",
      "14 0.7143880538038173\n",
      "15 0.7154319676800828\n",
      "16 0.7097366429164729\n",
      "17 0.7067603394464773\n",
      "18 0.7058744872277171\n",
      "19 0.7443789022555416\n",
      "20 0.7775803586761424\n",
      "21 0.7762757632392603\n",
      "22 0.7747033627363588\n",
      "23 0.7781361266069076\n",
      "24 0.783626005851184\n",
      "25 0.788986649470874\n",
      "26 0.7768568303450715\n",
      "27 0.7676763491307047\n",
      "28 0.7875435922075503\n",
      "29 0.7685812257518093\n",
      "30 0.7461966768516559\n",
      "31 0.7541715711196213\n",
      "32 0.7439933785816666\n",
      "33 0.7635545243688332\n",
      "34 0.7953988900566931\n",
      "35 0.7802354797650374\n",
      "36 0.7768875566808764\n",
      "37 0.7761849346229717\n",
      "38 0.8079974398336426\n",
      "39 0.8120313718200692\n",
      "40 0.8113655419591916\n",
      "41 0.8078333821925159\n",
      "42 0.8117695504163437\n",
      "43 0.8177050332178093\n",
      "44 0.808231075415247\n",
      "45 0.7989502032726911\n",
      "46 0.792618518044555\n",
      "47 0.813652076872137\n",
      "48 0.8321813346671677\n",
      "49 0.841282360638275\n",
      "50 0.8476112883990511\n",
      "51 0.8477998757267278\n",
      "52 0.845422005285235\n",
      "53 0.8509225453830664\n",
      "54 0.8494037787327068\n",
      "55 0.8538386350075999\n",
      "56 0.8503462984663182\n",
      "57 0.8424033973958649\n",
      "58 0.8386997461294683\n",
      "59 0.8451211238803955\n",
      "60 0.8442249196589459\n",
      "61 0.8347795167270524\n",
      "62 0.8320930611157602\n",
      "63 0.8209341383801526\n",
      "64 0.8429942091032983\n",
      "65 0.8605096654400309\n",
      "66 0.8709528278785102\n",
      "67 0.8588344520103474\n",
      "68 0.8689210454352114\n",
      "69 0.8605929054310429\n",
      "70 0.8449337314309542\n",
      "71 0.8406471109494078\n",
      "72 0.8359192947157844\n",
      "73 0.8367065811044093\n",
      "74 0.8299728883664695\n",
      "75 0.825526151269941\n",
      "76 0.8380819454234419\n",
      "77 0.8733541660347313\n",
      "78 0.8649333857001011\n",
      "79 0.8608854001082884\n",
      "80 0.8674556528233696\n",
      "81 0.8788897877332024\n",
      "82 0.8684807622657703\n",
      "83 0.8642534958802726\n",
      "84 0.870605171482725\n",
      "85 0.8773658556998538\n",
      "86 0.8696551117669855\n",
      "87 0.8848176114065891\n",
      "88 0.892805906106112\n",
      "89 0.8909105529418002\n",
      "90 0.887261607180623\n",
      "91 0.8773898264785861\n",
      "92 0.8530457303778086\n",
      "93 0.8342775835287024\n",
      "94 0.8389549079115748\n",
      "95 0.8391738345417751\n",
      "96 0.8641904099893359\n",
      "97 0.8605063896224961\n",
      "98 0.8671178306040085\n",
      "99 0.8585680428116405\n",
      "100 0.8660848576541582\n",
      "101 0.8779777947761644\n",
      "102 0.8643363406367793\n",
      "103 0.854645786707381\n",
      "104 0.8498878970264863\n",
      "105 0.8424273935866473\n",
      "106 0.8377834553592686\n",
      "107 0.8417996764716852\n",
      "108 0.8390559431516904\n",
      "109 0.8399941956952917\n",
      "110 0.8534782450660253\n",
      "111 0.8496951482339519\n",
      "112 0.8527753901241889\n",
      "113 0.8414699823822919\n",
      "114 0.8279022025092776\n",
      "115 0.8316935879535374\n",
      "116 0.8266913182483023\n",
      "117 0.8340392881544929\n",
      "118 0.8454573363448575\n",
      "119 0.8412254739783395\n",
      "120 0.828975991602201\n",
      "121 0.8424888342597356\n",
      "122 0.8450707936516119\n",
      "123 0.8394691220887104\n",
      "124 0.8343107150926838\n",
      "125 0.8143307684229346\n",
      "126 0.8218994573206383\n",
      "127 0.8338551751404925\n",
      "128 0.8470066986183894\n",
      "129 0.8453128917968166\n",
      "130 0.8563513154376448\n",
      "131 0.8632141340953785\n",
      "132 0.8616850860254645\n",
      "133 0.8564163462602379\n",
      "134 0.8446580171014477\n",
      "135 0.8222275337096465\n",
      "136 0.8386777196240549\n",
      "137 0.8266104265672664\n",
      "138 0.8429279340939798\n",
      "139 0.8358891312885973\n",
      "140 0.8460157567045308\n",
      "141 0.8362535160824609\n",
      "142 0.8458910606483924\n",
      "143 0.8343304958970471\n",
      "144 0.847297297953111\n",
      "145 0.8508198074058886\n",
      "146 0.8397662031881511\n",
      "147 0.8412758688631591\n",
      "148 0.8575900165205139\n",
      "149 0.8517091772574223\n",
      "150 0.8552117817280452\n",
      "151 0.8533319928031966\n",
      "152 0.8430447440327363\n",
      "153 0.8462160645509217\n",
      "154 0.8397268425132457\n",
      "155 0.8456334930262086\n",
      "156 0.8463482351284958\n",
      "157 0.8510830999074648\n",
      "158 0.8388360814930428\n",
      "159 0.8328956252156055\n",
      "160 0.8240874440204958\n",
      "161 0.8193795373918027\n",
      "162 0.8366898927677179\n",
      "163 0.8265150195586961\n",
      "164 0.8244726161062339\n",
      "165 0.82429251247494\n",
      "166 0.8191770357257351\n",
      "167 0.8207263186315295\n",
      "168 0.8152613037802731\n",
      "169 0.8137601484303253\n",
      "170 0.8261926084096131\n",
      "171 0.8125262569274609\n",
      "172 0.8166028142238025\n",
      "173 0.7918849742382725\n",
      "174 0.7968230387295163\n",
      "175 0.8052813534692334\n",
      "176 0.8150369628085368\n",
      "177 0.8093774885841798\n",
      "178 0.8108724509142132\n",
      "179 0.8133976002494592\n",
      "180 0.8123028743569662\n",
      "181 0.8095849645264477\n",
      "182 0.7928184499082199\n",
      "183 0.7877986790102685\n",
      "184 0.8046423210618091\n",
      "185 0.8015755789894643\n",
      "186 0.816496705781973\n",
      "187 0.7937524855944733\n",
      "188 0.7893558370603487\n",
      "189 0.7804869501175666\n",
      "190 0.7909680147307622\n",
      "191 0.7940823631595632\n",
      "192 0.8032745189792445\n",
      "193 0.797395170290001\n",
      "194 0.792224144194764\n",
      "195 0.7978477797725425\n",
      "196 0.7951062481226178\n",
      "197 0.7875460572720745\n",
      "198 0.7731596824780286\n",
      "199 0.7789151394052094\n"
     ]
    }
   ],
   "source": [
    "best_idx, best_score, best_weights = train(model, 200, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bdb4d3-a443-49dd-b6b8-8bd0e2eb180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(88), np.float64(0.892805906106112))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a5de1f-2cb8-4b95-be07-5593fdd89037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cdf3e29-9b5e-45f2-bd9a-ddb35e5d7d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8903345454723381)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b7f914-5df0-42f7-ba3f-66fd067fedbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4959)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_accuracy(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12357dc1-dffc-4cfe-bb2c-a43f68df6fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c4e53-113d-4c3e-9313-b49226fab9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360bff5-18f0-4498-8f39-609d65a87fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1c4ac-cba0-4a8a-ac78-219395bc1954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51526c42-3033-4bd4-901e-47736be9a71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9a23a38-6743-4a25-82f8-01bf9e9bc5e6",
   "metadata": {},
   "source": [
    "## 一些测试结论\n",
    "\n",
    "先做了一些模拟测试（10万次）确定评价模型的基准。如果采用最优解，单局游戏奖励的平均值为0.933，标准差0.347。采用随机策略的平均奖励是0.297。\n",
    "\n",
    "首先初步对比了一下有无 BatchNormalization 和 Dropout 的情况，差距颇大。无 BatchNormalization 和 Dropout 时表现很差。观察 weights，应该是遇到了梯度消失和梯度爆炸。\n",
    "\n",
    "之后的测试都用了 BatchNormalization 和 Dropout。\n",
    "\n",
    "\n",
    "### 超参数\n",
    "\n",
    "几个结论：\n",
    "1. 对比强化学习和监督学习，强化学习算法的实现方式是正确的。\n",
    "2. adam 比 sgd 更快，且更稳定（没有体现在上面的数据里），为什么？sgd模型的参数绝对值会越来越大，adam似乎没这个问题\n",
    "3. 对于两个隐藏层的模型，sgd 比 adam 表现更好\n",
    "4. 两个隐藏层的模型比一个隐藏层的模型效果差。为什么？\n",
    "5. 为什么增大模型规模能提升模型效果和加快学习速度？看到过一个理论，说模型中大部分参数是没用的，只有部分参数起决定性作用，因为这部分参数的初始值很好。大的模型有更多的参数，有更大机会命中较好的初始参数。\n",
    "\n",
    "之后调整测试 policy gradient method 采用超参数：optimalizer, n_hidden_layers, n_dense_units, ratio_dropout = 'adam', 1, 512, 0.5\n",
    "\n",
    "\n",
    "观察了两个强化学习和监督学习模型，各自的得分是0.89和0.9，差距很小，最优解的命中率却是0.49和0.77。考虑：\n",
    "1. 用 baseline 方法重新训练策略模型试试。\n",
    "2. 训练过程中逐步降温。\n",
    "\n",
    "\n",
    "### 降温\n",
    "\n",
    "理论上是错误做法，实际效果也不好。\n",
    "\n",
    "\n",
    "### 奖励 normalization\n",
    "\n",
    "考虑以下行为\n",
    "* 修改初始化时格子中随机数的取值区间\n",
    "* 在获得的奖励上减去常数\n",
    "* 在获得的奖励上乘以常数\n",
    "\n",
    "理论上它们不应该影响策略，实际中它们对模型表现有重大影响。\n",
    "\n",
    "所以，是否存在某个理想的适于模型学习的 G_t 的分布？\n",
    "\n",
    "尝试在一个batch内对奖励作 normalization，效果较好。\n",
    "\n",
    "\n",
    "* 有没有比 normalization 更好的办法？\n",
    "* 用一个batch内的奖励的平均值和标准差做 normalization 合适吗？如果 batch_size 很小，感觉似乎不合适\n",
    "\n",
    "怎么结合 normalization 和 baseline 呢？先在奖励上减去 baseline 再做 normalization？\n",
    "\n",
    "\n",
    "### win_or_lose\n",
    "\n",
    "命中最优解奖励1，否则-1。\n",
    "\n",
    "加上归一化后，命中最优解的几率达到0.66，不过平均每局得分不见优势。\n",
    "\n",
    "## 最大问题\n",
    "\n",
    "不收敛。模型表现一开始变好，然后越来越差。为什么？假如说我现在正处于不断变差的阶段，继续训练，表现为什么不会提升？奖励信号为什么失效了？\n",
    "\n",
    "尝试：\n",
    "* 用test_model返回值做normalization\n",
    "* 对每个state进行多次action抽样\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824bdb8-b2bd-48f6-97dd-3ce2f526a8ee",
   "metadata": {},
   "source": [
    "## baseline\n",
    "\n",
    "将 one_batch 中 \n",
    "```\n",
    "y_target[action] = run(state, action)\n",
    "```\n",
    "改为\n",
    "```\n",
    "y_target[action] = run(state, action) - 0.933\n",
    "```\n",
    "\n",
    "模型没有完全失效，训练过程中仍有提升，但只达到了0.72 左右。我看不出为什么在奖励上减去一个固定的数字会影响结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
