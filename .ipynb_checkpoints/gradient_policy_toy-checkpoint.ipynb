{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "424fc8e6-e309-4608-b270-a4fd7161eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.models import Model, clone_model\n",
    "from itertools import product\n",
    "\n",
    "LOW, HIGH = 0.0, 1.0\n",
    "\n",
    "def make_states(n):\n",
    "    states = np.zeros((n,8))\n",
    "    for i in np.arange(n):\n",
    "        idx = np.random.choice(8, size=3, replace=False)\n",
    "        states[i, idx] = np.random.uniform(LOW, HIGH, size=3)\n",
    "    return states\n",
    "    \n",
    "def run(state, action):\n",
    "    if state[action] != 0:\n",
    "        return state[action]\n",
    "    else:\n",
    "        before, after = state[(action - 1) % 8], state[(action + 1) % 8]\n",
    "        if before != 0 and after != 0:\n",
    "            return before + after\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def get_optimal_action_and_value(state):\n",
    "    values = np.array([run(state, action) for action in np.arange(8)])\n",
    "    optimal_action = values.argmax()\n",
    "    return optimal_action, values[optimal_action]\n",
    "\n",
    "def get_optimal_actions_and_values(state_list):\n",
    "    res = [get_optimal_action_and_value(state) for state in state_list]\n",
    "    actions = np.array([pairs[0] for pairs in res])\n",
    "    values = np.array([pairs[1] for pairs in res])\n",
    "    return actions, values\n",
    "    \n",
    "def get_optimal_statics(n_rounds):\n",
    "    state_list = make_states(n_rounds)\n",
    "    _, values = get_optimal_actions_and_values(state_list)\n",
    "    return np.mean(values), np.std(values)\n",
    "\n",
    "def get_random_policy_statics(n_rounds):\n",
    "    values = [run(state, np.random.choice(8)) for state in make_states(n_rounds)]\n",
    "    return np.mean(values), np.std(values)\n",
    "\n",
    "def get_model_actions(model, state_list):\n",
    "    return model(np.array(state_list)).numpy().argmax(axis = 1)\n",
    "\n",
    "def test_model(model, n_test_rounds):\n",
    "    state_list = make_states(n_test_rounds)\n",
    "    actions = get_model_actions(model, state_list)\n",
    "    values = [run(state, action) for state, action in zip(state_list, actions)]\n",
    "    optimal_actions, _ = get_optimal_actions_and_values(state_list)\n",
    "    accuracy = np.mean(actions == optimal_actions)\n",
    "    return np.mean(values), np.std(values), accuracy\n",
    "\n",
    "def one_batch(model, batch_size, n_test_rounds=0):\n",
    "    y_target_list = np.zeros((batch_size, 8))\n",
    "    state_list = make_states(batch_size)\n",
    "    prob_list = model(np.array(state_list)).numpy()\n",
    "    actions = np.array([np.random.choice(8, p=prob) for prob in prob_list])\n",
    "    values = np.array([run(state, action) for state, action in zip(state_list, actions)])\n",
    "    _, optimal_values = get_optimal_actions_and_values(state_list)\n",
    "    values = values - optimal_values\n",
    "    values = (values - np.mean(values)) / (np.std(values) + 1e-8)\n",
    "    for i in np.arange(batch_size):\n",
    "        y_target_list[i, actions[i]] = values[i]\n",
    "    model.fit(state_list, y_target_list, epochs=1, batch_size=batch_size, verbose = 0)\n",
    "    if n_test_rounds > 0:\n",
    "        return test_model(model, n_test_rounds)\n",
    "\n",
    "def one_batch_supervised(model, batch_size, n_test_rounds=0):\n",
    "    state_list = make_states(batch_size)\n",
    "    y_target_list = np.zeros((batch_size, 8))\n",
    "    optimal_actions, _ = get_optimal_actions_and_values(state_list)\n",
    "    for i in np.arange(batch_size):\n",
    "        y_target_list[i, optimal_actions[i]] = 1\n",
    "    model.fit(np.array(state_list), np.array(y_target_list), epochs=1, batch_size=batch_size, verbose=0)\n",
    "    if n_test_rounds > 0:\n",
    "        return test_model(model, n_test_rounds)\n",
    "\n",
    "def train(one_batch, model, max_batch=200, batch_size=128, n_test_rounds=10000, verbose = 0):\n",
    "    best_weights = []\n",
    "    best_idx = 0\n",
    "    best_score = 0\n",
    "    for i in np.arange(max_batch):\n",
    "        score, std, accuracy = one_batch(model, batch_size, n_test_rounds)\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_idx = i\n",
    "            best_weights = model.get_weights()\n",
    "        if verbose == 1:\n",
    "            print(i, score, accuracy)\n",
    "    return best_idx, best_score, best_weights\n",
    "\n",
    "def train_supervised(one_batch_supervised, model, max_batch=200, batch_size=128, n_test_rounds=10000, verbose = 0):\n",
    "    best_weights = []\n",
    "    best_idx = 0\n",
    "    best_score = 0\n",
    "    for i in np.arange(max_batch):\n",
    "        score, std, accuracy = one_batch_supervised(model, batch_size, n_test_rounds)\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_idx = i\n",
    "            best_weights = model.get_weights()\n",
    "        if verbose == 1:\n",
    "            print(i, score)\n",
    "    return best_idx, best_score, best_weights\n",
    "\n",
    "\n",
    "def create_model(n_hidden_layers, n_dense_units, ratio_dropout, optimizer):\n",
    "    input_shape = (8,) \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = inputs\n",
    "    for i in np.arange(n_hidden_layers):\n",
    "        x = Dense(n_dense_units)(x) \n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Dropout(ratio_dropout)(x)\n",
    "    \n",
    "    outputs = Dense(8, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5d4ac-5549-4dc6-b7cc-97e21e379fcb",
   "metadata": {},
   "source": [
    "# 单步奖励收集\n",
    "\n",
    "这是一个用于练习策略梯度法 (policy gradient method) 的玩具题。  \n",
    "8个格子首尾相连，其中随机3个格子奖励，奖励数值为0~1的均匀随机数。  \n",
    "动作：选择一个格子。\n",
    "计分规则：  \n",
    "1. 如果格子有奖励（大于0），获得对应的奖励。\n",
    "2. 如果格子是空的（等于0），且两侧的格子都有奖励，会获得两侧格子的总奖励。\n",
    "3. 否则奖励为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb980bc-15d7-4457-aaea-b4312895c248",
   "metadata": {},
   "source": [
    "## 试验场"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00400e9d-e300-49aa-acb8-346fc82940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_hidden_layers=1, n_dense_units=512, ratio_dropout=0.5, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc5697b6-cdc2-4bcc-aec8-016d8dd689b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8478646175654474 0.6338\n",
      "1 0.8509879983317603 0.6326\n",
      "2 0.8455333533438745 0.6284\n",
      "3 0.837055844077785 0.6247\n",
      "4 0.8510514737780546 0.6315\n",
      "5 0.8446183210212119 0.6298\n",
      "6 0.8461430372056797 0.6281\n",
      "7 0.8394324125476497 0.6237\n",
      "8 0.8417414600394233 0.6256\n",
      "9 0.8399797613033337 0.6217\n",
      "10 0.8417830714649859 0.6273\n",
      "11 0.8309298049873165 0.6215\n",
      "12 0.8456453375117766 0.6335\n",
      "13 0.8439592889357922 0.6417\n",
      "14 0.8427427111284628 0.6362\n",
      "15 0.848011827368914 0.6456\n",
      "16 0.8483297548688772 0.6448\n",
      "17 0.8553990913506945 0.6386\n",
      "18 0.8451176372540078 0.6343\n",
      "19 0.8424569040099532 0.6312\n",
      "20 0.8485820348358897 0.6309\n",
      "21 0.847524332661327 0.6278\n",
      "22 0.8369223338586891 0.6226\n",
      "23 0.843021981654488 0.6235\n",
      "24 0.8506109757835364 0.6418\n",
      "25 0.8452753676050353 0.6367\n",
      "26 0.837357587590032 0.6332\n",
      "27 0.847893282123941 0.6331\n",
      "28 0.8400311792379714 0.6323\n",
      "29 0.8435311737716055 0.6389\n",
      "30 0.837961941798909 0.6288\n",
      "31 0.8363933636968283 0.6265\n",
      "32 0.8354882268798094 0.6339\n",
      "33 0.8309822348322418 0.6385\n",
      "34 0.8341297535177641 0.6258\n",
      "35 0.8339757066178679 0.6248\n",
      "36 0.8345310273639747 0.6331\n",
      "37 0.835111772855181 0.6247\n",
      "38 0.8363023643987683 0.629\n",
      "39 0.8304154663815317 0.6219\n",
      "40 0.8400590584421307 0.6371\n",
      "41 0.8359828835215761 0.6248\n",
      "42 0.8344172230428841 0.6294\n",
      "43 0.8463117866694383 0.6307\n",
      "44 0.8405355391683049 0.6304\n",
      "45 0.8411789715429164 0.6331\n",
      "46 0.8491814055072229 0.6315\n",
      "47 0.8418268600181876 0.6336\n",
      "48 0.8361555759839326 0.629\n",
      "49 0.8400715618950919 0.6286\n",
      "50 0.8335607606900068 0.6198\n",
      "51 0.8371458745379434 0.6157\n",
      "52 0.8427132589599269 0.6231\n",
      "53 0.8381882676629088 0.6281\n",
      "54 0.8387100329655152 0.6366\n",
      "55 0.834427836675334 0.6162\n",
      "56 0.831601493101481 0.617\n",
      "57 0.836803497192509 0.6347\n",
      "58 0.8417317032769548 0.6273\n",
      "59 0.8436318837047889 0.6372\n",
      "60 0.8426291294795245 0.6352\n",
      "61 0.8408409144686775 0.6374\n",
      "62 0.836433262725137 0.64\n",
      "63 0.847729074606061 0.6426\n",
      "64 0.8512957841529584 0.6458\n",
      "65 0.8509985217700571 0.6559\n",
      "66 0.855656202097766 0.6476\n",
      "67 0.8511395463196199 0.6535\n",
      "68 0.8530332060712195 0.6561\n",
      "69 0.8542426247880958 0.6622\n",
      "70 0.8540338485220044 0.6533\n",
      "71 0.8531615637316605 0.6569\n",
      "72 0.8567968756961284 0.6511\n",
      "73 0.8499577920832686 0.66\n",
      "74 0.8497107654491454 0.6566\n",
      "75 0.8488401543180167 0.6588\n",
      "76 0.8510117444243613 0.6461\n",
      "77 0.8507399485905713 0.6475\n",
      "78 0.8400824973633416 0.6517\n",
      "79 0.8359451441209912 0.6407\n",
      "80 0.8458958998394671 0.6477\n",
      "81 0.8385004380267803 0.6364\n",
      "82 0.8437967257705321 0.6428\n",
      "83 0.8416499836436473 0.6455\n",
      "84 0.8475044671508997 0.6558\n",
      "85 0.8416836853411148 0.6455\n",
      "86 0.8449698561621635 0.6543\n",
      "87 0.8443315016491897 0.6471\n",
      "88 0.8445980250241529 0.652\n",
      "89 0.8555446957767091 0.659\n",
      "90 0.8380981405135289 0.6441\n",
      "91 0.8379670768861126 0.6396\n",
      "92 0.8337880554805506 0.6342\n",
      "93 0.8342381849143432 0.6338\n",
      "94 0.8293071312297494 0.6359\n",
      "95 0.8315696219295307 0.6351\n",
      "96 0.8378599490784847 0.6404\n",
      "97 0.8361275817145662 0.6326\n",
      "98 0.8275837779930046 0.6263\n",
      "99 0.835267976365534 0.6353\n",
      "100 0.8330169752813177 0.6318\n",
      "101 0.8363976250218402 0.6433\n",
      "102 0.8349426884554774 0.627\n",
      "103 0.8359006038786776 0.6387\n",
      "104 0.8432691917469741 0.6374\n",
      "105 0.8399498258879936 0.6343\n",
      "106 0.8405810346437059 0.6376\n",
      "107 0.8273586275728365 0.6299\n",
      "108 0.8385514447255964 0.6402\n",
      "109 0.8406125635506573 0.6356\n",
      "110 0.8417320443022575 0.6333\n",
      "111 0.8363709486074494 0.636\n",
      "112 0.8419195899704035 0.6418\n",
      "113 0.8351129007013426 0.6272\n",
      "114 0.8305244849028129 0.6162\n",
      "115 0.8347888059426765 0.6256\n",
      "116 0.8322294181195704 0.6199\n",
      "117 0.8321867269515824 0.6196\n",
      "118 0.8292571880950607 0.6241\n",
      "119 0.8311856341022456 0.6204\n",
      "120 0.8381158632618008 0.6268\n",
      "121 0.8299889875610447 0.6299\n",
      "122 0.8328178054662232 0.6214\n",
      "123 0.8264230514209044 0.6256\n",
      "124 0.8368594862198192 0.6134\n",
      "125 0.8370311438183115 0.6241\n",
      "126 0.8328216689919318 0.6332\n",
      "127 0.8347088635563836 0.634\n",
      "128 0.8351820643041874 0.6344\n",
      "129 0.8491420317968312 0.6504\n",
      "130 0.8459164578168036 0.6522\n",
      "131 0.8418370419491679 0.6522\n",
      "132 0.8473829669887949 0.6505\n",
      "133 0.8516368217070889 0.6555\n",
      "134 0.8460044850004775 0.6589\n",
      "135 0.8477528462823802 0.6633\n",
      "136 0.853139798738963 0.6629\n",
      "137 0.8489339202869934 0.6623\n",
      "138 0.8507219335902888 0.6585\n",
      "139 0.8554634690508894 0.6614\n",
      "140 0.8480834668702213 0.6624\n",
      "141 0.8362177548762518 0.6441\n",
      "142 0.8381584440941551 0.652\n",
      "143 0.8423483803456328 0.6501\n",
      "144 0.8471486891174064 0.6591\n",
      "145 0.838743807118683 0.6473\n",
      "146 0.8372869870768355 0.648\n",
      "147 0.8311839400848976 0.6388\n",
      "148 0.8280599322739656 0.6346\n",
      "149 0.8319246039093093 0.6272\n",
      "150 0.822457892099601 0.6227\n",
      "151 0.8259936513474233 0.619\n",
      "152 0.8146771455652535 0.5997\n",
      "153 0.825293942127053 0.6108\n",
      "154 0.8242781913433302 0.6087\n",
      "155 0.8348570286625385 0.6164\n",
      "156 0.8344873030175237 0.6187\n",
      "157 0.8356516245720117 0.6337\n",
      "158 0.8410716877021565 0.6371\n",
      "159 0.8417591510347 0.6356\n",
      "160 0.8436063785787293 0.6504\n",
      "161 0.837633121280909 0.6425\n",
      "162 0.8459960354878496 0.6448\n",
      "163 0.8297401039535319 0.6316\n",
      "164 0.815642738791216 0.6202\n",
      "165 0.82570951940938 0.6173\n",
      "166 0.8248694333086772 0.6284\n",
      "167 0.8218606028231781 0.6257\n",
      "168 0.8199019737035578 0.6245\n",
      "169 0.8197613579372585 0.621\n",
      "170 0.8073634850789443 0.6114\n",
      "171 0.8068827122908969 0.6123\n",
      "172 0.8059489950046465 0.6055\n",
      "173 0.8184547554644388 0.616\n",
      "174 0.8177096762032845 0.6163\n",
      "175 0.8154703217816736 0.6128\n",
      "176 0.8212534300147958 0.6205\n",
      "177 0.8315483472765103 0.627\n",
      "178 0.8202534531281932 0.6303\n",
      "179 0.8380373753877699 0.6446\n",
      "180 0.837334326831123 0.6434\n",
      "181 0.8409570177180609 0.6513\n",
      "182 0.8381464768993193 0.648\n",
      "183 0.8488627572007122 0.6631\n",
      "184 0.8490424135105566 0.6518\n",
      "185 0.8448432884289147 0.6586\n",
      "186 0.8502677454782235 0.6563\n",
      "187 0.8445128383509174 0.6567\n",
      "188 0.8362330284753136 0.6552\n",
      "189 0.8328403845347839 0.647\n",
      "190 0.8352730614177035 0.6484\n",
      "191 0.8421265482035677 0.6477\n",
      "192 0.828130878177444 0.6365\n",
      "193 0.8349549760193077 0.637\n",
      "194 0.8270334209144014 0.6335\n",
      "195 0.8291009687215037 0.6312\n",
      "196 0.8344393183372293 0.6443\n",
      "197 0.8386490550385828 0.6339\n",
      "198 0.8392112489775427 0.6463\n",
      "199 0.8373216602189932 0.641\n"
     ]
    }
   ],
   "source": [
    "best_idx, best_score, best_weights = train(one_batch, model, max_batch=200, verbose = 1)\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07bdb4d3-a443-49dd-b6b8-8bd0e2eb180b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(72),\n",
       " np.float64(0.8502460178006955),\n",
       " np.float64(0.3824965504654445),\n",
       " np.float64(0.6578))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx, *test_model(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5de1f-2cb8-4b95-be07-5593fdd89037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9855d99b-be4e-4159-9665-3d1c59c4aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_hidden_layers=1, n_dense_units=512, ratio_dropout=0.5, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41805807-c7cc-46b7-a878-0d7cf1d9011f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.40155409005897713\n",
      "1 0.43835942170670744\n",
      "2 0.47521455094780124\n",
      "3 0.5233936808251424\n",
      "4 0.5457541841190825\n",
      "5 0.60900033488207\n",
      "6 0.6395265413305641\n",
      "7 0.6578620451271016\n",
      "8 0.6911916340208485\n",
      "9 0.7070164593537791\n",
      "10 0.7151154144860967\n",
      "11 0.7321296068984608\n",
      "12 0.7251182328173948\n",
      "13 0.7309836764964616\n",
      "14 0.7589333375036049\n",
      "15 0.7682199268337767\n",
      "16 0.7675557165837719\n",
      "17 0.7607641742508313\n",
      "18 0.7641782073134503\n",
      "19 0.7755925855301341\n",
      "20 0.7770031560281603\n",
      "21 0.7829396475602843\n",
      "22 0.7956468143766171\n",
      "23 0.7972822214302536\n",
      "24 0.797411493945989\n",
      "25 0.7803849629307795\n",
      "26 0.7759102454787973\n",
      "27 0.7636392430081078\n",
      "28 0.770455245657486\n",
      "29 0.7799667565660786\n",
      "30 0.790556762839348\n",
      "31 0.7928761250297651\n",
      "32 0.7965587006606923\n",
      "33 0.8100902418202696\n",
      "34 0.8095332245864129\n",
      "35 0.8120959575341389\n",
      "36 0.8060285753678174\n",
      "37 0.8054377181837991\n",
      "38 0.7873472926820616\n",
      "39 0.8020201904664144\n",
      "40 0.813652638932629\n",
      "41 0.8246549172733206\n",
      "42 0.8207024059061694\n",
      "43 0.8216845070555435\n",
      "44 0.813161396662448\n",
      "45 0.8044061940287046\n",
      "46 0.8155602679091162\n",
      "47 0.8275687298301907\n",
      "48 0.8264218996044956\n",
      "49 0.8302477771974902\n",
      "50 0.8326108519367262\n",
      "51 0.8299460110452846\n",
      "52 0.8306033728716393\n",
      "53 0.8192077094146042\n",
      "54 0.8142928680219639\n",
      "55 0.844126406859097\n",
      "56 0.8477290574781877\n",
      "57 0.8507630654120545\n",
      "58 0.8519851704607232\n",
      "59 0.8501118405631045\n",
      "60 0.8534180810276139\n",
      "61 0.8443584501893915\n",
      "62 0.8503067370735619\n",
      "63 0.8511766921606704\n",
      "64 0.843516343206623\n",
      "65 0.8533110841844173\n",
      "66 0.8451169056162029\n",
      "67 0.8630461592428746\n",
      "68 0.8591926409745781\n",
      "69 0.8643175242551261\n",
      "70 0.8612165279155449\n",
      "71 0.8595812459287445\n",
      "72 0.8427843064987166\n",
      "73 0.8488487558460897\n",
      "74 0.8349339494380038\n",
      "75 0.8475048546275508\n",
      "76 0.8415943697395607\n",
      "77 0.8400643938021134\n",
      "78 0.8393168470606411\n",
      "79 0.8326696292106801\n",
      "80 0.8509156706248061\n",
      "81 0.8586205186230034\n",
      "82 0.8602995602286968\n",
      "83 0.8548377015557805\n",
      "84 0.8500037265791257\n",
      "85 0.8493475761798421\n",
      "86 0.8274242023573324\n",
      "87 0.8241942726832061\n",
      "88 0.8257519176996326\n",
      "89 0.8422291693422076\n",
      "90 0.8572856220009849\n",
      "91 0.8563746416914266\n",
      "92 0.8524866609107079\n",
      "93 0.8431113488577863\n",
      "94 0.8497725667505698\n",
      "95 0.8491243510849313\n",
      "96 0.8274084333945453\n",
      "97 0.832557534764054\n",
      "98 0.836520153648421\n",
      "99 0.8357067426175747\n",
      "100 0.8441481386094442\n",
      "101 0.8360686489702266\n",
      "102 0.8299122302518049\n",
      "103 0.828045889032844\n",
      "104 0.8370325407523515\n",
      "105 0.8214492632290057\n",
      "106 0.8280599470297464\n",
      "107 0.8362734649054674\n",
      "108 0.83588904931972\n",
      "109 0.8308928579924529\n",
      "110 0.8444198207109397\n",
      "111 0.8279638383522626\n",
      "112 0.8353085448118807\n",
      "113 0.8255360630090876\n",
      "114 0.8029102547211929\n",
      "115 0.80558658403417\n",
      "116 0.8181613068503977\n",
      "117 0.8176970449173064\n",
      "118 0.822683032438496\n",
      "119 0.8071808784423434\n",
      "120 0.8109392190545905\n",
      "121 0.8139597999387767\n",
      "122 0.8011685164284333\n",
      "123 0.8264909816074507\n",
      "124 0.8338408039688875\n",
      "125 0.8331041115730392\n",
      "126 0.8337191521588817\n",
      "127 0.8249528835770216\n",
      "128 0.8202282508795115\n",
      "129 0.7888836021056841\n",
      "130 0.7962695455107127\n",
      "131 0.8303235641656552\n",
      "132 0.8200050727423702\n",
      "133 0.8091205591609919\n",
      "134 0.8118800157723352\n",
      "135 0.8109172818324383\n",
      "136 0.8141378797809339\n",
      "137 0.8268465933652688\n",
      "138 0.8115135325752721\n",
      "139 0.7958184897129293\n",
      "140 0.8154319249693226\n",
      "141 0.8269319804040243\n",
      "142 0.8238522722864018\n",
      "143 0.8300092143714659\n",
      "144 0.8205279419873588\n",
      "145 0.8148453570521775\n",
      "146 0.8066200679550781\n",
      "147 0.8134758452621276\n",
      "148 0.8139922515958675\n",
      "149 0.8099923018218697\n",
      "150 0.7968030128411382\n",
      "151 0.7974101236762011\n",
      "152 0.7990107318021229\n",
      "153 0.7971520440353179\n",
      "154 0.780854879951367\n",
      "155 0.7945705285753891\n",
      "156 0.8007587383739662\n",
      "157 0.8049238478260001\n",
      "158 0.8028691952846655\n",
      "159 0.7855899499366423\n",
      "160 0.7994457599329094\n",
      "161 0.810712848510165\n",
      "162 0.8003180601010806\n",
      "163 0.8017540451492372\n",
      "164 0.7982601223985475\n",
      "165 0.7754392694510426\n",
      "166 0.7608091284206222\n",
      "167 0.7605846575453782\n",
      "168 0.7680729317765392\n",
      "169 0.7770174644711056\n",
      "170 0.7711148434458777\n",
      "171 0.7479053376405463\n",
      "172 0.759500433033828\n",
      "173 0.7769512974383671\n",
      "174 0.7604733731702665\n",
      "175 0.7690120344330695\n",
      "176 0.7873620025098982\n",
      "177 0.7665101627710886\n",
      "178 0.7709704708439976\n",
      "179 0.7842708270846435\n",
      "180 0.7775292788019811\n",
      "181 0.7565434102588404\n",
      "182 0.7748367317861812\n",
      "183 0.767693691748335\n",
      "184 0.7634084795852221\n",
      "185 0.7713316249764175\n",
      "186 0.7801195728973618\n",
      "187 0.7604792410648468\n",
      "188 0.7664424777168983\n",
      "189 0.7754437378776764\n",
      "190 0.7752726408475159\n",
      "191 0.7627172469398115\n",
      "192 0.7590739618282127\n",
      "193 0.7657209797562277\n",
      "194 0.7496802276640985\n",
      "195 0.7440508051899435\n",
      "196 0.7599107464061877\n",
      "197 0.7549631598806616\n",
      "198 0.7712419462484663\n",
      "199 0.7493157580741461\n"
     ]
    }
   ],
   "source": [
    "best_idx, best_score, best_weights = train(model, 200, verbose = 1)\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a494fdd-04c8-4001-872c-a7e402b4ddd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(69),\n",
       " np.float64(0.8608465176411966),\n",
       " np.float64(0.38633711000818116),\n",
       " np.float64(0.6628))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx, *test_model(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458ea4b-a558-4a78-9e6c-a6de934db57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9a23a38-6743-4a25-82f8-01bf9e9bc5e6",
   "metadata": {},
   "source": [
    "## 一些测试结论\n",
    "\n",
    "先做了一些模拟测试（10万次）确定评价模型的基准。如果采用最优解，单局游戏奖励的平均值为0.933，标准差0.347。采用随机策略的平均奖励是0.297。\n",
    "\n",
    "首先初步对比了一下有无 BatchNormalization 和 Dropout 的情况，差距颇大。无 BatchNormalization 和 Dropout 时表现很差。观察 weights，应该是遇到了梯度消失和梯度爆炸。\n",
    "\n",
    "之后的测试都用了 BatchNormalization 和 Dropout。\n",
    "\n",
    "\n",
    "### 超参数\n",
    "\n",
    "几个结论：\n",
    "1. 对比强化学习和监督学习，强化学习算法的实现方式是正确的。\n",
    "2. adam 比 sgd 更快，且更稳定（没有体现在上面的数据里），为什么？sgd模型的参数绝对值会越来越大，adam似乎没这个问题\n",
    "3. 对于两个隐藏层的模型，sgd 比 adam 表现更好\n",
    "4. 两个隐藏层的模型比一个隐藏层的模型效果差。为什么？\n",
    "5. 为什么增大模型规模能提升模型效果和加快学习速度？看到过一个理论，说模型中大部分参数是没用的，只有部分参数起决定性作用，因为这部分参数的初始值很好。大的模型有更多的参数，有更大机会命中较好的初始参数。\n",
    "\n",
    "之后调整测试 policy gradient method 采用超参数：optimalizer, n_hidden_layers, n_dense_units, ratio_dropout = 'adam', 1, 512, 0.5\n",
    "\n",
    "\n",
    "观察了两个强化学习和监督学习模型，各自的得分是0.89和0.9，差距很小，最优解的命中率却是0.49和0.77。考虑：\n",
    "1. 用 baseline 方法重新训练策略模型试试。\n",
    "2. 训练过程中逐步降温。\n",
    "\n",
    "\n",
    "### 降温\n",
    "\n",
    "理论上是错误做法，实际效果也不好。\n",
    "\n",
    "\n",
    "### 奖励 normalization\n",
    "\n",
    "考虑以下行为\n",
    "* 修改初始化时格子中随机数的取值区间\n",
    "* 在获得的奖励上减去常数\n",
    "* 在获得的奖励上乘以常数\n",
    "\n",
    "理论上它们不应该影响策略，实际中它们对模型表现有重大影响。\n",
    "\n",
    "所以，是否存在某个理想的适于模型学习的 G_t 的分布？\n",
    "\n",
    "尝试在一个batch内对奖励作 normalization，效果较好。\n",
    "\n",
    "\n",
    "* 有没有比 normalization 更好的办法？\n",
    "* 用一个batch内的奖励的平均值和标准差做 normalization 合适吗？如果 batch_size 很小，感觉似乎不合适\n",
    "* 用test_model返回值做normalization？实际效果不如在batch内做normalization\n",
    "\n",
    "### baseline\n",
    "\n",
    "以最优得分为baseline，在奖励上减去 baseline 再做 normalization。对于初始化时随机数区间较大的问题，表现有较大提升，且较为稳定。\n",
    "\n",
    "\n",
    "### win_or_lose\n",
    "\n",
    "命中最优解奖励1，否则-1。\n",
    "\n",
    "加上归一化后，命中最优解的几率达到0.7左右，不过平均每局得分不见优势。\n",
    "\n",
    "## 最大问题\n",
    "\n",
    "不收敛。模型表现一开始变好，然后越来越差。为什么？假如说我现在正处于不断变差的阶段，继续训练，表现为什么不会提升？奖励信号为什么失效了？\n",
    "\n",
    "尝试对每个state进行多次action抽样，无效。生成了一组样本，反复训练，在同组样本上测试，模型表现仍然会变差。\n",
    "\n",
    "on-polity 测试模型表现？表现同样会逐渐变差\n",
    "\n",
    "## 学习 gemini \n",
    "\n",
    "gemini 的训练过程很稳定，似乎没有越练越差。\n",
    "\n",
    "## tf.GradientTape vs model.fit\n",
    "\n",
    "看来 model.fit 和 tf.GradientTape 的差异是关键所在。网上搜了一下，似乎挺多人遇到过同样的问题，但有一些可能是代码写错了导致的。\n",
    "\n",
    "model.fit 默认 batch_size=32，奇特的是，不手动指定batch_size 和指定 batch_size=32，模型的训练表现大不相同。\n",
    "\n",
    "使用model.fit\n",
    "* 不指定batch_size时模型在训练过程中非常不稳定，越训练越差。\n",
    "* 指定batch_size=32，模型在训练过程中较为稳定，在较长的训练之后似乎有变差的倾向\n",
    "* 指定batch_size=128，比设为32更加稳定，在较长的训练之后表现出变差的倾向\n",
    "\n",
    "使用 tf.GradientTape，一次使用128个样本，比model.fit(batch_size=128)表现更稳定，训练速度稍慢，但后期表现更好，也没有显出变差的倾向\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1565e6f-67e1-42cb-8ae2-c4e568fc50bc",
   "metadata": {},
   "source": [
    "## gemini给的代码\n",
    "\n",
    "出于效率考虑 evaluate_policy 及训练中调用model计算policy几率的部分要改一下\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Game parameters\n",
    "num_cells = 8\n",
    "num_reward_cells = 3\n",
    "reward_range = (5.0, 10.0)\n",
    "\n",
    "# Neural network parameters\n",
    "hidden_layer_size = 64\n",
    "batch_size = 128\n",
    "num_batches = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the policy network\n",
    "model = Sequential([\n",
    "    Dense(hidden_layer_size, input_shape=(num_cells,), kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    Dense(num_cells, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Function to generate a game\n",
    "def generate_game():\n",
    "    game_state = np.zeros(num_cells)\n",
    "    reward_indices = np.random.choice(num_cells, num_reward_cells, replace=False)\n",
    "    game_state[reward_indices] = np.random.uniform(reward_range[0], reward_range[1], num_reward_cells)\n",
    "    return game_state\n",
    "\n",
    "# Function to calculate reward\n",
    "def calculate_reward(game_state, action):\n",
    "    if game_state[action] > 0:\n",
    "        return game_state[action]\n",
    "    else:\n",
    "        left_index = (action - 1) % num_cells\n",
    "        right_index = (action + 1) % num_cells\n",
    "        if game_state[left_index] > 0 and game_state[right_index] > 0:\n",
    "            return game_state[left_index] + game_state[right_index]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Function to evaluate the policy\n",
    "def evaluate_policy(model, num_episodes=10000):\n",
    "    total_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        game_state = generate_game()\n",
    "        probs = model(np.expand_dims(game_state, axis=0)).numpy()[0]\n",
    "        action = np.random.choice(num_cells, p=probs)\n",
    "        total_reward += calculate_reward(game_state, action)\n",
    "    return total_reward / num_episodes\n",
    "\n",
    "# Training loop\n",
    "rewards_history = []\n",
    "for batch in range(num_batches):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        game_state = generate_game()\n",
    "        states.append(game_state)\n",
    "        probs = model(np.expand_dims(game_state, axis=0)).numpy()[0]\n",
    "        action = np.random.choice(num_cells, p=probs)\n",
    "        actions.append(action)\n",
    "        reward = calculate_reward(game_state, action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Normalize rewards\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(np.array(states))\n",
    "        action_probs = tf.gather_nd(probs, [[i, a] for i, a in enumerate(actions)])\n",
    "        log_probs = tf.math.log(action_probs)\n",
    "        loss = -tf.reduce_mean(log_probs * rewards)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Evaluate the policy\n",
    "    average_reward = evaluate_policy(model)\n",
    "    rewards_history.append(average_reward)\n",
    "\n",
    "    if (batch + 1) % 10 == 0:\n",
    "        print(f\"Batch: {batch + 1}/{num_batches}, Average Reward: {average_reward:.3f}\")\n",
    "\n",
    "# Plot the average reward over batches\n",
    "plt.plot(range(num_batches), rewards_history)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Policy Gradient Training\")\n",
    "plt.show()\n",
    "```\n",
    "## 修改后的\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Game parameters\n",
    "num_cells = 8\n",
    "num_reward_cells = 3\n",
    "reward_range = (5.0, 10.0)\n",
    "\n",
    "# Neural network parameters\n",
    "hidden_layer_size = 512\n",
    "batch_size = 128\n",
    "num_batches = 10000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the policy network\n",
    "model = Sequential([\n",
    "    Dense(hidden_layer_size, input_shape=(num_cells,), kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_cells, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Function to generate a game\n",
    "def generate_game():\n",
    "    game_state = np.zeros(num_cells)\n",
    "    reward_indices = np.random.choice(num_cells, num_reward_cells, replace=False)\n",
    "    game_state[reward_indices] = np.random.uniform(reward_range[0], reward_range[1], num_reward_cells)\n",
    "    return game_state\n",
    "\n",
    "# Function to calculate reward\n",
    "def calculate_reward(game_state, action):\n",
    "    if game_state[action] > 0:\n",
    "        return game_state[action]\n",
    "    else:\n",
    "        left_index = (action - 1) % num_cells\n",
    "        right_index = (action + 1) % num_cells\n",
    "        if game_state[left_index] > 0 and game_state[right_index] > 0:\n",
    "            return game_state[left_index] + game_state[right_index]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# # Function to evaluate the policy\n",
    "# def evaluate_policy(model, num_episodes=10000):\n",
    "#     total_reward = 0\n",
    "#     for _ in range(num_episodes):\n",
    "#         game_state = generate_game()\n",
    "#         probs = model(np.expand_dims(game_state, axis=0)).numpy()[0]\n",
    "#         action = np.random.choice(num_cells, p=probs)\n",
    "#         total_reward += calculate_reward(game_state, action)\n",
    "#     return total_reward / num_episodes\n",
    "\n",
    "\n",
    "# Function to evaluate the policy\n",
    "def evaluate_policy(model, num_episodes=10000):\n",
    "    total_reward = 0\n",
    "    states = np.array([generate_game() for i in np.arange(num_episodes)])\n",
    "    actions = model(states).numpy().argmax(axis = 1)\n",
    "    for game_state, action in zip(states, actions):\n",
    "        total_reward += calculate_reward(game_state, action)\n",
    "    return total_reward / num_episodes\n",
    "\n",
    "\n",
    "# Training loop\n",
    "rewards_history = []\n",
    "for batch in range(num_batches):\n",
    "    states = np.array([generate_game() for i in np.arange(batch_size)])\n",
    "    actions = [np.random.choice(num_cells, p=probs) for probs in model(states).numpy()]\n",
    "    rewards = [calculate_reward(game_state, action) for game_state, action in zip(states, actions)]\n",
    "\n",
    "    # Normalize rewards\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(np.array(states))\n",
    "        action_probs = tf.gather_nd(probs, [[i, a] for i, a in enumerate(actions)])\n",
    "        log_probs = tf.math.log(action_probs)\n",
    "        loss = -tf.reduce_mean(log_probs * rewards)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Evaluate the policy\n",
    "    average_reward = evaluate_policy(model)\n",
    "    rewards_history.append(average_reward)\n",
    "\n",
    "    if (batch + 1) % 10 == 0:\n",
    "        print(f\"Batch: {batch + 1}/{num_batches}, Average Reward: {average_reward:.3f}\")\n",
    "\n",
    "# Plot the average reward over batches\n",
    "plt.plot(range(num_batches), rewards_history)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Policy Gradient Training\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de5573-079a-46bf-ae39-67641fb1fb24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
