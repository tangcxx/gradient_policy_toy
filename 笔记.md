# policy gradient method 笔记

## 一个简单的练习问题

这是一个单步游戏。有8个格子围成一圈，其中3个格子里加入0.0 - 1.0的均匀随机数，剩下5个为0。

可以采取的动作是选择一个格子。

计分规则：
* 如果所选格子中的数字大于0：得分为格子中的数字。否则，
* 如果所选格子左右两侧格子中的数字都大于0：得分为两侧格子中的数字相加。否则
* 得分为0。

我有意设计这样一个简单的问题用于学习 policy gradient method.

### 比较的基准

首先需要确定的是算法实现没有问题，因此需要有比较的基准来判断模型是否学到了知识。

两个自然的基准：
* 随机策略：平均得分0.297
* 最优解：平均得分0.933，标准差0.347

上述数字通过模拟测试获得。

### model.fit vs tf.GradientTape

试验过程中发现 model.fit 表现不正常。

policy gradient 模型更新参数的计算公式和使用交叉熵作为 loss function 的多分类监督学习模型更新参数的计算公式是一样的，只要把 loss function 设为交叉熵，把 y_target 设为 reward，就可以用 model.fit 来更新模型参数。model.fit 写起来更简便，而且由于我并不了解自动微分(automatic differentiation)技术，本能地就抗拒使用 tf.GradientTape。

在实验中遇到了一个顽固的无法理解的问题：模型在训练过程中一开始表现上升，但是继续训练会越来越差。

模型表现提升，说明我的方法和实现应该没问题。但是为什么继续训练会变差呢？问了一些 AI，它们认为可能的原因是过拟合或过大的 learning rate，这和我所遇到的情况不符。在反复检查了代码和各种超参数后，我对用 model.fit 投机取巧产生了怀疑：会不会是我对算法理解有误？于是我用 tf.GradientTape 重写代码。这次没问题了。

google搜索发现很多人都遇到过 tf.GradientTape 和 model.fit 表现不一致的情况。有一些是代码写得有问题，还有一些看上去似乎是真的有问题，但并没有看到合理的解释和解决方案。

stacksoverflow有个帖子提到 model.fit 默认 batch_size=32。这个帖子启发我做了一下测试。

使用model.fit：
* 不指定batch_size时模型在训练过程中非常不稳定，越训练越差。
* 指定batch_size=32，模型在训练过程中较为稳定，在较长的训练之后表现出变差的倾向。
* 指定batch_size=128，比设为32更加稳定，在较长的训练之后表现出变差的倾向。

model.fit 默认 batch_size=32，然而使用默认设置和手动设置batch_size=32，结果却有明显的差异。这使我对 model.fit 失去了信心。

虽然指定 batch_size 后模型在训练过程中的表现更稳定，但仍然比不上 tf.GradientTape。

此外，使用 model.fit，似乎训练速度比使用 tf.gradientTape 要快一点，前期更快地达到较高水平，不过这可能是因为不够稳定造成的错觉。

model.fit 的问题不知道是怎么造成的。它不是完全不对，它仍然能更新模型，提升模型效果到一定水平。此外用最优解做监督学习时，没发现它有问题。猜测它可能采取了某些优化措施，因而带来了负作用。使用 model.fit 需要指定 loss function，在用于 policy gradient时，loss function 应当指定为交叉熵，但它实际不是交叉熵，事实上对于强化学习来说根本就不存在 loss function。使用 model.fit 和交叉熵是一种取巧的手段。

### 一些结论

超参数的测试是在定位 model.fit 的问题之前做的。不过因为结论很平庸，我想应该是同样适用于 tf.GradientTape 的。

* adam 很好，无脑用 adam。
* BatchNormalization 和 Dropout 很好，必须用。
* rewards 的 baseline 和 normalization 很有用。对于格子中奖励数字在0.0 - 1.0之间的情况，效果还不明显，对于其他更大的取值区间（比如5.0 - 10.0），效果很明显。
* 降温：理论上是错误做法，实际效果也不好。
* 用是否命中最优解作输赢判断：训练出来的模型有更高几率命中最优解，但得分不占优势。
* 网络规模：不是越大越好，也不是越小越好。对于这样一个简单的问题，凭感觉，一个很小的神经网络应该就有足够的参数来描述它，但实际上还是稍大一点更容易训练。有一种说法是，神经网络中只有一部分参数是有用的，它们在初始化时运气比较好，而大的网络在初始化的时候有更高几率命中好的参数。

## 同时训练 policy network 和 value network

